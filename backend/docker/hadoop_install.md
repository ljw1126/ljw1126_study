## ì°¸ê³  
**wikidocs - ë¬´ìŠ¨ ì±… ì‚¬ì´íŠ¸ ê°™ìŒ**
[https://wikidocs.net/22654](https://wikidocs.net/22654)

**DATA ON-AIR**
[https://dataonair.or.kr/db-tech-reference/d-guide/data-practical/?mod=document&uid=402](https://dataonair.or.kr/db-tech-reference/d-guide/data-practical/?mod=document&uid=402)

**ğŸ‘¨â€ğŸ’»Apache hadoop ëª…ë ¹ì–´ ê³µì‹ ë¬¸ì„œ**
[https://hadoop.apache.org/docs/](https://hadoop.apache.org/docs/)
[https://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-common/FileSystemShell.html](https://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-common/FileSystemShell.html)

**ssh config ì„¤ì •/ì ‘ì† ê´€ë ¨**
[https://blog.jiniworld.me/106](https://blog.jiniworld.me/106)

**ec2 ë¡œ í•˜ëŠ” ì˜ˆì œì¸ë° ì œì¼ ì˜ë˜ì–´ ìˆìŒ .. slave1/2 ì™œ í†µì‹  ì•ˆë˜ì—ˆëŠ”ì§€ ì´ìœ ë¥¼ ì—¬ê¸°ì„œ ì°¾ìŒ**
[https://1mini2.tistory.com/84](https://1mini2.tistory.com/84)

**(ì‚¬ìš©ì•ˆí•¨) docker hadoop ì…‹íŒ… ë‹¤ ë˜ì–´ìˆëŠ”ê²Œ ìˆëŠ”ë“¯**
[https://github.com/big-data-europe/docker-hadoop](https://github.com/big-data-europe/docker-hadoop)

**í•˜ë‘¡ íŠ¸ëŸ¬ë¸” ìŠˆíŒ… ê´€ë ¨**
[https://www.cs.brandeis.edu/~cs147a/lab/hadoop-troubleshooting/](https://www.cs.brandeis.edu/~cs147a/lab/hadoop-troubleshooting/)

**í•˜ë‘¡ v1.x ì„¤ì¹˜ ì˜ìƒ** 
[https://www.youtube.com/watch?v=KDC2Nto4NfE](https://www.youtube.com/watch?v=KDC2Nto4NfE)

**í•˜ë‘¡ NameNode ê¸°ë™ê³¼ì •ê³¼ ë©”ì»¤ë‹ˆì¦˜ì˜ ì´í•´**
[https://likebnb.tistory.com/162](https://likebnb.tistory.com/162)

---
// START

## í•˜ë‘¡(Hadoop)ì´ë€?
```
í•˜ë‘¡ì€ 2006ë…„ ì•¼í›„ì˜ ë”ê·¸ ì»¤íŒ…ì´ 'ë„›ì¹˜'ë¼ëŠ” ê²€ìƒ‰ì—”ì§„ì„ ê°œë°œí•˜ëŠ” ê³¼ì •ì—ì„œ ëŒ€ìš©ëŸ‰ì˜ ë¹„ì •í˜• ë°ì´í„°ë¥¼ ê¸°ì¡´ì˜ RDB ê¸°ìˆ ë¡œëŠ” ì²˜ë¦¬ê°€ í˜ë“¤ë‹¤ëŠ” ê²ƒì„ ê¹¨ë‹«ê³ , ìƒˆë¡œìš´ ê¸°ìˆ ì„ ì°¾ëŠ” ì¤‘ êµ¬ê¸€ì—ì„œ ë°œí‘œí•œ GFSì™€ MapReduce ê´€ë ¨ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ì—¬ ê°œë°œí•˜ì˜€ìŠµë‹ˆë‹¤. ì´í›„ ì•„íŒŒì¹˜ ì¬ë‹¨ì˜ ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ê³µê°œ ë˜ì—ˆìŠµë‹ˆë‹¤.

í•˜ë‘¡ì€ í•˜ë‚˜ì˜ ì„±ëŠ¥ ì¢‹ì€ ì»´í“¨í„°ë¥¼ ì´ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ëŒ€ì‹ , ì ë‹¹í•œ ì„±ëŠ¥ì˜ ë²”ìš© ì»´í“¨í„° ì—¬ëŸ¬ ëŒ€ë¥¼ í´ëŸ¬ìŠ¤í„°í™”(êµ°ì§‘í™”*)í•˜ê³ , í° í¬ê¸°ì˜ ë°ì´í„°ë¥¼ í´ëŸ¬ìŠ¤í„°ì—ì„œ ë³‘ë ¬ë¡œ ë™ì‹œì— ì²˜ë¦¬í•˜ì—¬ ì²˜ë¦¬ ì†ë„ë¥¼ ë†’ì´ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•˜ëŠ” ë¶„ì‚°ì²˜ë¦¬ë¥¼ ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```

> edit logë‘ Standby NameNodeë¼ëŠ”ê±¸ ë§Œë“¤ì–´ë‘¬ì„œ ê¸°ì¡´ NameNode ì¥ì•  ë°œìƒì‹œ ë°±ì—… ëŒ€ì²˜ ê°€ëŠ¥í•˜ë„ë¡ ë²„ì „ì—…ì´ ë˜ì—ˆë‹¤ë‚˜ ë­ë¼ë‚˜

## Hadoop ì™€ HDFS êµ¬ë¶„ 
[https://1mini2.tistory.com/85](https://1mini2.tistory.com/85)

```
í•˜ë‘¡ì´ë€?
ëŒ€ëŸ‰ì˜ ìë£Œë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì»´í“¨í„° í´ëŸ¬ìŠ¤í„°ì—ì„œ ë™ì‘í•˜ëŠ” "í”„ë¦¬ì›¨ì–´ ìë°” ì†Œí”„íŠ¸ì›¨ì–´ í”„ë ˆì„ì›Œí¬"ì…ë‹ˆë‹¤.
Apache Hadoop Frameworkì—ì„œëŠ” ì•„ë˜ì˜ ëª¨ë“ˆì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.
  - í•˜ë‘¡ ì»¤ë¨¼(Hadoop Common)
  - í•˜ë‘¡ ë¶„ì‚° íŒŒì¼ ì‹œìŠ¤í…œ(HDFS) - ìŠ¤í† ë¦¬ì§€ ê³„ì¸µ
  - í•˜ë‘¡ YARN - ìì› ê´€ë¦¬ ê³„ì¸µ
  - í•˜ë‘¡ ë§µë¦¬ë“€ìŠ¤ - ì²˜ë¦¬ ê³„ì¸µ 

ì°¸ê³ ë§í¬ (https://ko.wikipedia.org/wiki/%EC%95%84%ED%8C%8C%EC%B9%98_%ED%95%98%EB%91%A1)

HDFSë€? 
  í•˜ë‘¡ í´ëŸ¬ìŠ¤í„°ì˜ ë°ì´í„° ìŠ¤í† ë¦¬ì§€ ê³„ì¸µì…ë‹ˆë‹¤.
  í•˜ë‘¡ í”„ë ˆì„ì›Œí¬ë¥¼ ìœ„í•´ ìë°”ì–¸ì–´ë¡œ ì‘ì„±ëœ ë¶„ì‚° í™•ì¥ íŒŒì¼ì‹œìŠ¤í…œ ì…ë‹ˆë‹¤.
  ë§¤ìš° í° íŒŒì¼ì„ ì €ì¥í•˜ë„ë¡ ì„¤ê³„(ëŒ€ìš©ëŸ‰íŒŒì¼ ë¶„ì‚° ì €ì¥)ë˜ì—ˆìœ¼ë©°, ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë…¸ë“œì— ì¤‘ë³µí•´ì„œ ì €ì¥í•©ë‹ˆë‹¤.

HDFSëŠ” í•˜ë‘¡ì˜ ìŠ¤í† ë¦¬ì§€ ê³„ì¸µìœ¼ë¡œ, Hadoop ë¶„ì‚°íŒŒì¼ì‹œìŠ¤í…œ(hdfs)ë¥¼ ë§í•©ë‹ˆë‹¤. :)
ë‹¤ì‹œ ì •ë¦¬í•´ ë§í•˜ìë©´, hdfsëŠ” í•˜ë‘¡ í”„ë ˆì„ì›Œí¬ì˜ í•œ ë¶€ë¶„ì¸ê±°ì£ ! 

```



## NameNode(master)/DataNode(slave) 
> NameNodeì™€ DataNodeì˜ ê°œìˆ˜ë¥¼ ì–´ë–»ê²Œ ì•Œì§€ ?

## Hadoopì„ ì„¤ì¹˜í•˜ë©´ hdfs/yarn/mapreduceê°€ ê¸°ë³¸ì ìœ¼ë¡œ ê¹”ë ¤ìˆëŠ”ê±´ê°€? ì–´ë–»ê²Œ ì•™ã„¹ì§€?

## Hadoop fs ì™€ dfs ì°¨ì´ 
- hadoop fs ì˜ ê²½ìš° 
  - local fs ë‚˜ hdfs ë“±ê³¼ ê°™ì´ì—¬ëŸ¬ íŒŒì¼ ì‹œìŠ¤í…œê³¼ ìƒí˜¸ ì‘ìš©í•  ìˆ˜ ìˆëŠ” ëª…ë ì–´ ì´ë‹¤. 
- hadoop dfs 
  - hdfsì—ë§Œ í•´ë‹¹/ì‚¬ìš©ê°€ëŠ¥ -> ì´ëŠ” ë”ì´ìƒ ì‚¬ìš© ë˜ì§€ ì•Šê³  **hdfs dfs** ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨  
- hdfs dfs 
  - hdfsì—ë§Œ í•´ë‹¹/ì‚¬ìš©ê°€ëŠ¥

> ìš”ì•½í•˜ìë©´ hadoop fs ëŠ” hadoop ë¿ë§Œì•„ë‹ˆë¼ ì—¬ëŸ¬ íŒŒì¼ì‹œìŠ¤í…œê³¼ í˜¸í™˜ê°€ëŠ¥ ëª…ë ¹ì–´ê³ , hdfs dfsëŠ” HDFSì—ë§Œ í•´ë‹¹í•˜ëŠ” ëª…ë ¹ì–´ì´ë‹¤.

[https://reference-m1.tistory.com/197]

## ì›¹ê¸°ë°˜ ì„œë¹„ìŠ¤ ì²´í¬ 

  **YARN ë¦¬ì†ŒìŠ¤ ë§¤ë‹ˆì € ì›¹í˜ì´ì§€**
    YARNì—ì„œ ì‹¤í–‰ë˜ëŠ” ëª¨ë“  ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒíƒœ ì›¹ì„œë¹„ìŠ¤(Web UI)ë¥¼ í™•ì¸ê°€ëŠ¥
    > http://localhost:8088              
  
  **HDFS íŒŒì¼ ì‹œìŠ¤í…œì˜ ì›¹í˜ì´ì§€**
    YARN ê¸°ë°˜ì˜ HDFS ìƒíƒœì— ê´€í•œ ì›¹ ì„œë¹„ìŠ¤(Web UI)ë¥¼ í™•ì¸ê°€ëŠ¥
    > http://localhost:50070

## HDFS / YARN ì„œë¹„ìŠ¤ êµ¬ë¶„ 
> jps  
  
  HDFS ì„œë¹„ìŠ¤ : NameNode, SecondaryNameNode, DataNode
  YARN ì„œë¹„ìŠ¤ : ResourceManager, NodeManager 
     

## hadoop ì¬ì‹œì‘ì‹œ putìœ¼ë¡œ ì˜¬ë¦° ë°ì´í„° ë‹¤ ì‚¬ë¼ì§€ëŠ”ê°€ ?

## HDFS / YARN ê°œë³„ ì¬ì‹œì‘ì‹œ NameNode ì‹¤í–‰ì´ ì•ˆë˜ëŠ”ë° ì–´ë–»ê²Œ ì²˜ë¦¬ ê°€ëŠ¥ ? 

## í•˜ë‘¡ í•µì‹¬ ì„¤ì •íŒŒì¼ 
core-site.xml
  $HADOOP_HOME/etc/hadoop/core-site.xml     

hdfs-site.xml
  $HADOOP_HOME/etc/hadoop/hdfs-site.xml

mapred-site.xml
  $HADOOP_HOME/hadoop-2.10.2/etc/hadoop

yarn-site.xml 
  $HADOOP_HOME/etc/hadoop


## bash_profileê³¼ bashrcì˜ ì°¨ì´ì  
  .bash_profile : ë¡œê·¸ì¸ ì‰˜ í™˜ê²½ (ì‚¬ìš©ì ë¡œê·¸ì¸, ssh, su - í• ë•Œë§Œ ë¡œë“œ)
  .bashrc : ë¡œê·¸ì¸ ì´ì™¸ì˜ ì‰˜ í™˜ê²½ (í„°ë¯¸ë„ ì°½ì„ ì—´ë•Œ, bash ì‰˜ì— ì ‘ê·¼í• ë•Œ ë¡œë“œ)

// END
---


## ë””ë ‰í† ë¦¬ í™•ì¸
```
$ hadoop fs -ls 
```

## íŒŒì¼ ì‚­ì œ 
```
$ hadoop fs -rm [íŒŒì¼]
```

## ë””ë ‰í† ë¦¬ ì‚­ì œ 
```
$ hadoop fs -rmr output 
```

## í•˜ë‘¡ì‹œìŠ¤í…œì— íŒŒì¼ ì €ì¥
```
$ hadoop fs -put ëŒ€ìƒíŒŒì¼ .       // ëŒ€ìƒíŒŒì¼ì„ í•˜ë‘¡ì˜ ë£¨íŠ¸ ê²½ë¡œì— ì €ì¥ 
```

- hadoop ì€ ì‘ì€ íŒŒì¼ì— ëŒ€í•œ ì²˜ë¦¬ê°€ ìµœì í™” ë˜ì–´ìˆëŠ”ê²Œ ì•„ë‹ˆë¼, í° íŒŒì¼ì„ ë³‘ë ¬ë¡œ ë¶„ì‚°ì²˜ë¦¬ í•˜ëŠ”ë° ìµœì í™” ë˜ì–´ ìˆëŠ” ì‹œìŠ¤í…œ.
- hadoop ì€ ìë°” ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ì–´ì ¸ ìˆìŒ ( php, python, shell script ë“±ìœ¼ë¡œë„ ì œì–´ê°€ëŠ¥ )
- hadoop ë””ë ‰í† ë¦¬ì— hadoop-examples-1.0.4.jar 
- fs* : file system ì œì–´

## í•˜ë‘¡ì‹œìŠ¤í…œì— ì €ì¥ëœ íŒŒì¼ì„ mapreduceë¡œ 
- í•´ì„ : hadoop-examples-1.0.4.jar íŒŒì¼ì— ìˆëŠ” wordcount í´ë˜ìŠ¤ë¥¼ ì‹¤í–‰ì‹œì¼œì„œ [ë¶„ì„íŒŒì¼]ì— ëŒ€í•œ ê²°ê³¼ë¥¼ [ë””ë ‰í† ë¦¬(ì—†ìœ¼ë©´ í˜„ì¬ ê²½ë¡œ ê¸°ì¤€ìœ¼ë¡œ ìƒì„±)]ì— ì €ì¥í•˜ê² ë‹¤.
```
$ hadoop jar hadoop-examples-1.0.4.jar wordcount [ë¶„ì„íŒŒì¼] [ë””ë ‰í† ë¦¬]
$ hadoop jar hadoop-examples-1.0.4.jar wordcount [ë¶„ì„íŒŒì¼] wordcount_deadpoetssociety
$ hadoop fs -ls       // í™•ì¸ 
$ hadoop fs -ls wordcount_deadpoetssociety    // í´ë” ë‚´ìš© í™•ì¸ , _SUCCESS, _logsëŠ” ì˜ë¯¸ì—†ê³  part-r-00000 íŒŒì¼ì— ê²°ê³¼ ì €ì¥ë¨
```

## ë¶„ì„ ê²°ê³¼ í™•ì¸ 
```
$ hadoop fs -cat [ë””ë ‰í† ë¦¬]/[íŒŒì¼ëª…]
```

## í•˜ë‘¡ ì„¤ì¹˜ ì‚¬ì´íŠ¸
[https://mungiyo.tistory.com/17](https://mungiyo.tistory.com/17)

```
// ubuntu/centos ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸° 
$ docker search ubuntu 
- search : docker hubì—ì„œ ì´ë¯¸ì§€ ê²€ìƒ‰ 

$ docker pull ubuntu       // docker hub ì—ì„œ í™•ì¸í•œ ì´ë¯¸ì§€ NAME , STARSê°€ ë§ê³  OFFICIALì¸ ê±¸ ë°›ìŒ 

// ì‹¤í–‰ 
$ docker run -it --name ubuntu_server ubuntu
- i(interactive), -i(Pseudo-tty)
ë˜ëŠ” 
$ docker run -i -t --name ubuntu_server ubuntu /bin/bash
  docker run -i -t --name hello ubuntu /bin/bash
- /bin/bash ì¦‰ì‹œ ì‹¤í–‰

// git bash ë¡œ ì‹¤í–‰ì‹œ ì•ˆë˜ì„œ create ì‚¬ìš©í–ˆëŠ”ë° ì•ˆë¨..($ docker create -i -t --name ubuntu_server ubuntu /bin/bash)
ì—ëŸ¬. the input device is not a TTY.  If you are using mintty, try prefixing the command with 'winpty'
ì›ì¸. git bash ì‚¬ìš©í•´ì„œ ê·¸ëŸ¼.. cmd ì‚¬ìš©í•´ì„œ ëª…ë ¹ì–´ ì…ë ¥ì‹œ ì •ìƒ ì‹¤í–‰ë¨. gitìœ¼ë¡œ í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì†Œ ì°¸ê³ 
> winpty docker exec -it [ì»¨í…Œì´ë„ˆëª…] /bin/bash 
ì°¸ê³ . https://wotres.tistory.com/entry/docker-error-%ED%95%B4%EA%B2%B0%EB%B2%95-the-input-device-is-not-a-TTY-If-you-are-using-mintty-try-prefixing-the-command-with-winpty


    $ docker images            // ë‚´ë ¤ ë°›ì€ ì´ë¯¸ì§€ í™•ì¸ 
    $ docker ps -a             // ps : í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ , -a : ì „ë¶€  
    $ docker start [ì»¨í…Œì´ë„ˆëª…]      // ì»¨í…Œì´ë„ˆ ì‹¤í–‰
    $ docker stop [ì»¨í…Œì´ë„ˆëª…]
    $ docker attach [ì»¨í…Œì´ë„ˆëª…]     // ì‹¤í–‰ ì¤‘ì¸ ì»¨í…Œì´ë„ˆ ì ‘ì† 
    $ docker rm [ì»¨í…Œì´ë„ˆëª…]    // ì»¨í…Œì´ë„ˆ ì‚­ì œ 
    $ docker rmi [ì´ë¯¸ì§€ëª…]     // ì´ë¯¸ì§€ ì‚­ì œ 

// ìš°ë¶„íˆ¬ ì—…ë°ì´í„° 
# apt-get update 

// ìš°ë¶„íˆ¬ ì—…ê·¸ë ˆì´ë“œ 
# apt-get upgrade

// ìš°ë¶„íˆ¬ ì ‘ì† 
docker exec -it {container_id or name} /bin/bash    //ì»¨í…Œì´ë„ˆ ì ‘ì†í•˜ê¸° ëª…ë ¹ì–´  

// JDK ì„¤ì¹˜ 
> apt-get install openjdk-11-jdk    <-> ì‚­ì œì˜ ê²½ìš° apt-get purge openjdk*

> java -version
> javac -version

// JAVA_HOME í™˜ê²½ë³€ìˆ˜ ì„¤ì • 
> vi ~/.bashrc 

----------------------------
export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))
export PATH=$PATH:$JAVA_HOME/bin
----------------------------

> source ~/.bashrc 
> echo $JAVA_HOME      // í™˜ê²½ë³€ìˆ˜ ì„¤ì • í™•ì¸


```

## ping ìœ í‹¸ ì„¤ì¹˜ 
```
$ apt-get install iputils-ping -y
```

## ìš°ë¶„íˆ¬ì— vim ì„¤ì¹˜ 
```
$ apt-get install vim  -y    // root ì•„ë‹Œ ê²½ìš° ì•ì— sudo ë¶™ì´ê¸°
```

## netstat ì„¤ì¹˜ (netstat, ifconfig ë“± ì‚¬ìš© ê°€ëŠ¥)
```
> apt-get install net-tools
```

## zip, unzip ì„¤ì¹˜ 
```
> apt-get install -y zip unzip
```

##  ìš°ë¶„íˆ¬ ssh ì„œë²„ êµ¬ì¶• 
https://davelogs.tistory.com/17?category=933085
```
> apt-get install openssh-server 
> dpkg -l | grep openssh    //ê´€ë ¨ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸
> apt-get install openssh-clients openssh-askpass -y    // ì¶”ê°€ ì„¤ì¹˜

> service --status-all | grep + // ì‹¤í–‰ ì¤‘ì¸ ì„œë¹„ìŠ¤ ëª©ë¡ í™•ì¸ 
> netstat -tnlp                 // í¬íŠ¸ í™•ì¸ (22) 

//ì›ê²© ì ‘ì† 
ssh [id]@[address]
```

## vim ìë™ì •ë ¬ ë”˜ì¸¡í‚¤
https://wookiist.dev/1

## í•˜ë‘¡ì„¤ì¹˜  // ìš°ë¶„íˆ¬ 
```
> apt-get install wget 

// https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.10.2/hadoop-2.10.2.tar.gz

> wget https://dlcdn.apache.org/hadoop/common/hadoop-2.10.2/hadoop-2.10.2.tar.gz
> tar xvzf [ì••ì¶•íŒŒì¼ëª…]
> vi ~/.bashrc 

------------------------------------------------------------------------------------------
    export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))
    export PATH=$PATH:$JAVA_HOME/bin
    export JAVA_OPTS="-Dfile.encoding=UTF-8"
    export CLASSPATH="." 
    export HADOOP_HOME=/home/hadoop_home/hadoop-2.10.2
    export HADOOP_CONFIG_HOME=$HADOOP_HOME/etc/hadoop
    export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
    //export PATH=$PATH:$HADOOP_HOME/sbin
------------------------------------------------------------------------------------------

> source ~/.bashrc

// íŒŒì¼ ë‚´ìš© ìˆ˜ì • 
1. hadoop-env.sh

$ vim /home/hadoop_home/hadoop-2.10.2/etc/hadoop/hadoop-env.sh

```
  //export JAVA_HOME=${JAVA_HOME}     // í™˜ê²½ë³€ìˆ˜ ì„¤ì •ë˜ ì‡ìœ¼ë©´ ì•ˆí•´ë„ ë ë“¯ -> ìœ„ì—êº¼ ì£¼ì„ ì²˜ë¦¬ í•´ë²„ë¦¼ 

  // ì•„ë˜ configuration ì„ ì¶”ê°€ í›„ ì €ì¥í•˜ì
  export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
  export HDFS_NAMENODE_USER="root"
  export HDFS_DATANODE_USER="root"
  export HDFS_SECONDARYNAMENODE_USER="root"
  export YARN_RESOURCEMANAGER_USER="root"
  export YARN_NODEMANAGER_USER="root"

```

// ê° ë°ëª¬ë“¤ì´ ì‚¬ìš©í•  í™ˆ ë””í™í† ë¦¬ ìƒì„±
$ cd /home/hadoop_home
$ mkdir temp 
$ mkdir datanode 
$ mkdir namenode

// í™˜ê²½ì •ë³´ ì„¤ì • íŒŒì¼ 3ê°œ ìˆ˜ì • (core-site.xml, core-site.xml, mapred-site.xml)
$ cd $HADOOP_CONFIG_HOME 

1. core-site.xml 
- HDFSì™€ MapReduce ì—ì„œ ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©í•  í™˜ê²½ì •ë³´
- íŒŒì¼ ìœ„ì¹˜ : /home/hadoop_home/hadoop-2.10.2/etc/hadoop/core-site.xml     
- ë§¤ê°œë³€ìˆ˜ ì •ë³´ : [https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/core-default.xml](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/core-default.xml)

------------------------------------------------------------------------------------------
<configuration>
    <property>
            <name>hadoop.tmp.dir</name>
            <value>/root/soft/apache/hadoop/hadoop-2.7.7/tmp</value>     >> /home/hadoop_home/temp ê²½ë¡œ ìˆ˜ì • í›„ ì €ì¥
            <description>A base for other temporary directories.</description>
    </property>

    <property>
            <name>fs.default.name</name>           >> í•´ë‹¹ propertyëŠ” deprecated ë˜ì—ˆë„¤.. --> fs.defaultFS ì‚¬ìš©í•˜ê¸¸
            <value>hdfs://master:9000</value>     >> hdfs://master:9000 ìœ¼ë¡œ ìˆ˜ì •      , localhost ë¬¸ì œìˆìŒ..
            <final>true</final>
            <description>The name of the default file system.  A URI whose
            scheme and authority determine the FileSystem implementation.  The
            uri's scheme determines the config property (fs.SCHEME.impl) naming
            the FileSystem implementation class.  The uri's authority is used to
            determine the host, port, etc. for a filesystem.</description>
    </property>
</configuration>
------------------------------------------------------------------------------------------

2. hdfs-site.xml
- HDFSì—ì„œ ì‚¬ìš©í•  í™˜ê²½ ì •ë³´ 
- íŒŒì¼ ìœ„ì¹˜ : /home/hadoop_home/hadoop-2.10.2/etc/hadoop/hdfs-site.xml
- ë§¤ê°œë³€ìˆ˜ ì •ë³´ : [https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml](https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml)

------------------------------------------------------------------------------------------
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>3</value>      >> 1ë¡œ ìˆ˜ì • -> 3 
        <final>true</final>
        <description>Default block replication.
        The actual number of replications can be specified when the file is created.
        The default is used if replication is not specified in create time.
        </description>
    </property>

    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/root/soft/apache/hadoop/hadoop-2.7.7/namenode</value>     >> /home/hadoop_home/namenode ìˆ˜ì • í›„ ì €ì¥
        <final>true</final>
    </property>

    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/root/soft/apache/hadoop/hadoop-2.7.7/datanode</value>    >> /home/hadoop_home/datanode ìˆ˜ì • í›„ ì €ì¥
        <final>true</final>
    </property>
</configuration>
------------------------------------------------------------------------------------------

3. mapred-site.xml  -- íŒŒì¼ì´ ì—†ê³  í…œí”Œë¦¿ë§Œ ìˆì–´ì„œ ë³µì‚¬ í›„ ì‚¬ìš©
- MapReduceì—ì„œ ì‚¬ìš©í•  í™˜ê²½ ì •ë³´
- íŒŒì¼ ìœ„ì¹˜ : /home/hadoop_home/hadoop-2.10.2/etc/hadoop
- ë§¤ê°œë³€ìˆ˜ ì •ë³´ : [https://hadoop.apache.org/docs/r2.7.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml](https://hadoop.apache.org/docs/r2.7.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml)


$ cd /home/hadoop_home/hadoop-2.10.2/etc/hadoop
$ cp mapred-site.xml.template mapred-site.xml
$ vi mapred-site.xml 

------------------------------------------------------------------------------------------
<configuration>
    <property>

        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>

    <property>

        <name>mapred.job.tracker</name>
        <value>master:9001</value>
        <description>The host and port that the MapReduce job tracker runs
        at.  If "local", then jobs are run in-process as a single map
        and reduce task.
        </description>
    </property>

        <!-- job ë‚´ì—­ í™•ì¸ ê°€ëŠ¥í•œ web ui ì ‘ì†ê°„ìœ¼ : localhost:19888-->
        <property>
                <name>mapreduce.jobhistory.webapp.address</name>
                <value>master:19888</value>
        </property>
    
</configuration>
------------------------------------------------------------------------------------------

4. yarn-site.xml 
- ë§¤ê°œë³€ìˆ˜ ì •ë³´ : [https://hadoop.apache.org/docs/r2.7.6/hadoop-yarn/hadoop-yarn-common/yarn-default.xml](https://hadoop.apache.org/docs/r2.7.6/hadoop-yarn/hadoop-yarn-common/yarn-default.xml)

------------------------------------------------------------------------------------------
<configuration>

<!-- Site specific YARN configuration properties -->
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address</name>
    <value>master:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>master:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>master:8031</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>master:8033</value>
  </property>
  <property>
    <name>yarn.resourcemanager.webapp.adress</name>
    <value>master:8088</value>
  </property>
</configuration>

------------------------------------------------------------------------------------------

// ë„¤ì„ë…¸ë“œ í¬ë§· 
> hadoop namenode -format 

// ssh ì„¤ì¹˜ 
> apt-get install ssh -y 

// ~/.bashrc ìˆ˜ì • 
#autorun  
/usr/sbin/sshd

> source ~/.bashrc

> exit  // exit í•˜ë©´ docker container êº¼ì§ -> ctlr + pq í•˜ë©´ ê·¸ëƒ¥ ë‚˜ê°€ì§
> docker restart ubuntu_server 
> docker commit -m "hadoop install in ubuntu" ubuntu_server ubuntu_hadoop:hadoop      // ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ëŠ” êµ¬ë‚˜ 
> docker images 

// master, slave ìƒì„± 
docker run -i -t -h master --name master -p 50070:50070 -p 8088:8088 ubuntu_hadoop:hadoop 
docker run -i -t -h slave1 --name slave1 --link master:master ubuntu_hadoop:hadoop 
docker run -i -t -h slave2 --name slave2 --link master:master ubuntu_hadoop:hadoop 

// --link ì— ëŒ€í•´ ê°„ëµ ì„¤ëª…í•˜ìë©´ salve1ì€ masterì™€ linkë¥¼ ë§ºê²Œë˜ê²Œ ì»¨í…Œì´ë„ˆ ì´ë¦„ìœ¼ë¡œ í†µì‹ ê°€ëŠ¥ (https://it-sunny-333.tistory.com/85)
// slave1 ì— ì ‘ì† í›„ ping master í•˜ë©´ í˜¸ì¶œë¨ <-> ë°˜ëŒ€ë¡œ masterì—ì„œ slave1,2 pingìœ¼ë¡œ í˜¸ì¶œ ê°€ëŠ¥í•¨
> cat /etc/hosts 

  172.17.0.2      master master

> docker ps -a     // ì‹¤í–‰í™•ì¸

// master, slave ì»¨í…Œì´ë„ˆ ipí™•ì¸ 
docker inspect master | grep IPAddress   
docker inspect slave1 | grep IPAddress   // ë­”ê°€ ip ê²€ì‚¬ëŠ” ì•ˆë˜ë„¤.. 172.17.0.4
docker inspect slave2 | grep IPAddress   // 172.17.0.5

// í•˜ë‘¡ ì„¤ì • ë° êµ¬ë™
docker attach master          // ì»¨í…Œì´ë„ˆ êµ¬ë™ì¤‘ì´ì—¬ì•¼ í•¨ 


**cd /home/hadoop_home/hadoop-2.10.2/sbin/**

./start-all.sh // ì´ê±° í•´ë„ ì—ëŸ¬..
./stop-all.sh  // ì´ê±° í•´ë„ ì—ëŸ¬

 //ìê¾¸ ì—ëŸ¬ ë°œìƒ  -> ì´ê±°ëŠ” hosts ì— ì„ ì–¸ ì•ˆí•œê±´ë°.. ë¹„ë°€ë²ˆí˜¸ í‹€ë¦¬ëŠ”ê±´ stack overflow ë´¤ì„ë•Œ ssh ì ‘ì† í•˜ë„ë¡ í•´ë¼ë„¤ 
 slave2: ssh: Could not resolve hostname slave2: Name or service not known                                                     
 slave1: ssh: Could not resolve hostname slave1: Name or service not known                                                     

vi /etc/hosts 

  // ë‚´ìš© ì¶”ê°€
  172.17.0.3  master
  172.17.0.4  slave1
  172.17.0.5  slave2

// stop-all.shë‚˜ start-all.sh ì‹¤í–‰ì‹œ root password ì˜¤ë¥˜ê°€ ê³„ì† ë°œìƒí•  ê²½ìš° 
// https://stackoverflow.com/questions/15195048/hadoop-require-roots-password-after-enter-start-all-sh
// ë™ì¼í•œ ë‚´ìš© https://mungiyo.tistory.com/17

$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_dsa
$ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
$ ssh localhost       // ì—°ê²° í™•ì¸ , ê·¼ë° ë­ ubuntu ê°€ minimizedë˜ì—ˆë‹¤ê³  ì„¤ëª…ë§Œ ëœ¨ê³  ì‹¤í–‰ì´ ì•ˆë¨ .. 

$ unminimize          // ì„¤ëª…ì— ì í˜€ìˆëŠ” í‚¤ì›Œë“œ ì…ë ¥í•˜ë©´ ë­ ê³„ì† ëœ¸ 
$ ssh localhost       // ë­”ê°€ ë‹¤ë¦„. ì„¤ëª…ì„œ ì—†ê³  ë²„ì „ì´ë‘ Last login : ë‚ ì§œ ì¶œë ¥ë¨ 

$ cd $HADOOP_HOME/sbin 
$ ./stop-all.sh        // ë˜ ì—ëŸ¬ ëœ¨ë„¤ 

  Error : JAVA_HOME is not set and could not be found        ---> hadoop-env.sh íŒŒì¼ì— JAVA_HOME ìƒë‹¨êº¼ë¡œëŠ” ì•ˆë˜ë‹ˆ.. ë°‘ì— í•˜ë“œì½”ë”© ë°•ì•„ë²„ë¦¼ --> ì—ëŸ¬ ì œê±°ë¨
  localhost : Error : JAVA_HOME is not set and could not be found.

  slave2 : Host key verification failed.
  slave1 : Host key verification failed.

// ì´ê²ƒì €ê²ƒ ì„¤ì • ë³€ê²½í–ˆìœ¼ë‹ˆ .. 
$ hadoop namenode -format      // ctrl + pqë¡œ ë‚˜ê° 
$ docker commit -m "hadoop updated in master" master ubuntu_hadoop:hadoop      // ë„ì»¤ ì´ë¯¸ì§€ ê°±ì‹ ì„ í•´ì£¼ì..

// ì˜ë¬¸ì¸ê²Œ master imagesë¥¼ commit í•´ì„œ ì¬ì‹œì‘í•˜ëŠ”ë°.. slave1, slave2 ì„¤ì •ì´ ê°±ì‹ ë ê¹Œ??
$ docker restart master 
$ docker restart slave1
$ docker restart slave2
$ dcoker exec -it slave1 /bin/bash // í™•ì¸ì°¨ slave1ì— ì ‘ì†í•´ë³´ì --> /home/hadoop í´ë”ì— ì•„ë¬´ê²ƒë„ ì—†ë„¤ìš”..

$ docker stop slave1
$ docker stop slave2
$ docker rm slave1     // slave1 ì»¨í…Œì´ë„ˆ ì‚­ì œ
$ docker rm slave2 
$ docker ps -a       // ì»¨í…Œì´ë„ˆ ëª©ë¡ í™•ì¸

// slave1, slave2 ì¬ ìƒì„± --> vim /home/hadoop_home/hadoop-2.10.2/etc/hadoop/hadoop-env.sh í™•ì¸í•´ë³´ë‹ˆ masterë‘ ë™ì¼
$ docker run -i -t -h slave1 --name slave1 --link master:master ubuntu_hadoop:hadoop 
$ docker run -i -t -h slave2 --name slave2 --link master:master ubuntu_hadoop:hadoop 

$ dokcer exec -it master /bin/bash 
$ ./start-all.sh         // all daemon start 

  // vi /etc/hosts ì— slave1,2 ì—†ì–´ì§..  
  slave2 : ssh : could not resolve hostname slave2 : name or service not known
  slave1 : ssh : could not resolve hostname slave1 : name or service not known

  // hosts ìˆ˜ì •í›„ ìœ„ì—ê»€ ì—†ì–´ì¡ŒëŠ”ë° ì´ë ‡ë„¤.. 
  The authenticity of host 'slave1 (172.17.0.4)' can't be established
  The authenticity of host 'slave2 (172.17.0.5)' can't be established

  ED25519 key fingerprint is SHA256:eWNfxpdvzBCBlgVcO58vynK0eL1Y/HwhUJ/WK1Rf1lY.
  This host key is known by the following other names/addresses:
       ~/.ssh/known_hosts:1: [hashed name]
       ~/.ssh/known_hosts:2: [hashed name]
       ~/.ssh/known_hosts:3: [hashed name]
  slave2: Host key verification failed.
  slave1: Host key verification failed.



$ cd $HADOOP_HOME/sbin
$ ./stop-all.sh

  // ì§•ê¸€ì§•ê¸€í•˜ë‹¤.. 
  The authenticity of host 'slave1 (172.17.0.4)' can't be established
  The authenticity of host 'slave2 (172.17.0.5)' can't be established

$ hadoop jar /home/hadoop_home/hadoop-2.10.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.2.jar wordcount /home/hadoop_home/hadoop-2.10.2/LICENSE.txt wordcount_output  > log.txt 2>&1

  // ì—ëŸ¬1) ì§•ê¸€ì§•ê¸€í•˜ë‹¤.. 
  22/07/05 03:47:09 INFO client.RMProxy: Connecting to ResourceManager at master/172.17.0.3:8032
  java.net.ConnectException: Call From master/172.17.0.3 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused 

  // ì—ëŸ¬2) core-site.xml í¬íŠ¸ë¥¼ 9000 -> 8088 ìˆ˜ì •í•˜ë‹ˆ ìƒˆë¡œìš´ ì—ëŸ¬ 
  java.io.IOException: Failed on local exception: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length;
  Host Details : local host is: "master/172.17.0.3"; destination host is: "localhost":8088;

  > íŒíŠ¸ https://stackoverflow.com/questions/49060244/exception-org-apache-hadoop-ipc-rpcexception-rpc-response-exceeds-maximum-da
  > í¬íŠ¸ëŠ” 9000, 8020ë§Œ ê°€ëŠ¥í•˜ë‹¤í•´ì„œ .. core-site.xmlì— localhost:9000 í•˜ê³  start-all.sh ì¬ì‹œì‘í•´ì„œ ëŒë¦¬ë‹ˆ ìƒˆë¡œìš´ ì—ëŸ¬ í™•ì¸

  // ì—ëŸ¬3) ì´ê²Œ dfsì— íŒŒì¼ì„ ì•ˆì˜¬ë ¤ì„œ ê·¸ë ‡ë‹¤ëŠ”ë°? ê·¸ë˜ì„œ ë°‘ì— ì˜ˆì œ ì²˜ëŸ¼ í´ë”/íŒŒì¼ ì˜¬ë¦¬ê³  í•˜ë©´ ë™ì‘í•¨âœ¨
  22/07/05 04:32:32 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/root/.staging/job_1656989/hadoop-2.10.2/LICENSE.txt 
  org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://localhost:9000/home/hadoop_home/hadoop-2.10.2/LICENSE.txt

// slave1, slave2 ì ‘ì†í•´ì„œ /etc/hostsì— ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì‚­ì œ


$ jps     //  ìµœì¢…ê²°ê³¼ ì´ëŸ°ì‹ìœ¼ë¡œ ë°ëª¬ ì‹¤í–‰ ëª©ë¡ í™•ì¸
1185 Jps
486 SecondaryNameNode
663 DataNode
807 NodeManager
299 ResourceManager
140 NameNode

// HDFSì— LICENSE.txt íŒŒì¼ ì˜¬ë¦¼ 
$ hadoop fs -mkdir /anderson
$ hadoop fs -put LICENSE.txt /anderson               // ë¡œì»¬ íŒŒì¼ì‹œìŠ¤í…œì˜ íŒŒì¼ì„ HDFSì˜ ë¶„ì‚°íŒŒì¼ ì‹œìŠ¤í…œì— ì—…ë¡œë“œ
$ hadoop fs -ls /anderson 
-rw-r--r--   1 root supergroup     106210 2022-07-05 04:54 /anderson/LICENSE.txt

$ hadoop /home/hadoop_home/hadoop-2.10.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.2.jar wordcount /anderson /anderson_result > log.txt 2>&1
$ hadoop fs -ls /anderson_result       // ëª©ë¡ í™•ì¸
$ hadoop fs -cat /anderson_result     // ê²°ê³¼ ë³´ê¸° .. ë­”ê°€ ë‚˜ì˜¤ê¸´ í–ˆë„¤
or 
$ hadoop fs -text /anderson_result/part-r-0000          // ì´ê²Œ ê²°ê³¼íŒŒì¼

// ê·¸ì™¸ 
$ hadoop fs -get /adnerson_result   //í´ë”ì±„ë¡œ ë¶„ì‚°íŒŒì¼ ì‹œìŠ¤í…œ ìƒì˜ íŒŒì¼ì„ ë¡œì»¬ë¡œ ê°€ì ¸ì˜´ 

/*
  //2.10.1 ë²„ì „ì€ í¬íŠ¸ê°€ 19888ì¸ë“¯
  //MapReduce JobHistory Server Web UI host:port	Default port is 19888.
  > docker run -it --name hadoop-base -p 19888:19888 ubuntu_hadoop:hadoop // start-all.sh ì¬ì‹œì‘ í–ˆëŠ”ë° ì•ˆë¨ .. 
*/
ğŸ˜…localhost:8088 ì ‘ì†í•˜ë‹ˆ ë­”ê°€ hadoop cluster í˜ì´ì§€ ì ‘ì†ë¨ ! 
```


## ubuntu root ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” 
> passwd root  // í†µê³„ëŠ”êµ¬ë¼ë‹¤!!

--- 
## Linux Shell script - Redirection(>, >>, 2>&1)
[https://etloveguitar.tistory.com/m/20](https://etloveguitar.tistory.com/m/20)

## linux history ëª…ë ¹ì–´
[https://withcoding.com/107](https://withcoding.com/107)


-------


## ëˆ„ë½ëœ ì„¤ì •ë“¤ (https://1mini2.tistory.com/84)
```

1. Master ì—ì„œ slaves ì„¤ì • ëˆ„ë½ 

  # vi $HADOOP_HOME/etc/hadoop/slaves             
    ì „ì²´ ê²½ë¡œ :: /home/hadoop_home/hadoop-2.10.2/etc/hadoop/slaves

    v1.x í•˜ë‘¡ì˜ ê²½ìš° slaves íŒŒì¼ì— 
    [ë§ˆìŠ¤í„° ip ì£¼ì†Œ]
    [ì²«ë²ˆì§¸ ìŠ¬ë ˆì´ë¸Œ IPì£¼ì†Œ]
    [ë‘ë²ˆì§¸ ìŠ¬ë ˆì´ë¸Œ IPì£¼ì†Œ]
    

    -- localhost  ì§€ì›Œë²„ë¦¬ë˜ê°€
    master             // êµ­ë£°ì¸ê°€ë´„(?), ê²½ìœ í•œë‹¤ë©´..
    slave1
    slave2

2. master, slave1/2 ê°ê° ì ìš©
  2-1. slave1, 2 ì„œë²„ì— hosts ìˆ˜ì • 

    172.17.0.3  master master       // ì´ê±°ëŠ” docker run í• ë•Œ --link ì˜µì…˜ ì¤˜ì„œ ì´ëŸ¼
    172.17.0.4  slave1
    172.17.0.5  slave2

  2-2. slave ì„œë²„ì— ssh í‚¤ ìƒì„± ë° ssh ì„¤ì • ë³€ê²½ 
    $ ssh-keygen -t rsa -P '' -f ~/.ssh/id_dsa
    $ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
    $ ssh localhost       

    $ vi /etc/ssh/sshd_config
      // ì£¼ì„ í•´ì œ
      PermitRootLogin yes       
      PasswordAuthentication yes

    //$ systemctl restart sshd  // ëŒ€ëª¬ ì¬ì‹œì‘ .. ì•ˆë˜ë„¤..  sudo service sshd  restart ì•ˆë¨
    $ sudo passwd root      // í†µê³„ëŠ”êµ¬ë¼ë‹¤!!

    // sshd ì¬ì‹œì‘ ê´€ë ¨.. 
    apt-get install openssh-server 
    netstat -tnlp | grep sshd       // ì‹¤í–‰ í™•ì¸ 
    /etc/init.d/ssh restart         // ë™ì‘í™•ì¸ë¨

    # docker restart slave1  // slave2ë„ ì„¤ì • í›„ ì¬ì‹œì‘

3. Master ì„œë²„ ì ‘ì†í›„ slave1, slave2 ì™€ í‚¤ êµí™˜
   // master ì—ì„œë§Œ ìˆ˜í–‰ 
   $ ssh-copy-id -f root@slave1 
   $ ssh-copy-id -f root@slave2

4. Master ì„œë²„ëŠ” NameNode í¬ë§·, SlaveëŠ” DataNode í¬ë§· ì‹¤í–‰ 
   # Master ì„œë²„ì—ì„œ 
    $ hadoop namenode -format       // hadoop ëª…ë ¹ì–´ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ë° ì—†ìœ¼ë©´ /usr/local/hadoop-2.10.1/bin/hdfs namenode -format ìœ¼ë¡œ ì‹¤í–‰ 

   # NameNode ì„œë²„ì—ì„œ 
    $ hadoop datanode -format 

  //ê°ê° ì‹¤í–‰ í›„ /data í´ë”ê°€ ìƒì„±ëœë‹¤í•¨   

5. HDFS , YARN ì¬ì‹œì‘ 

  # í•œêº¼ë²ˆì— ì‹œì‘ 
    $ cd $HADOOP_HOME/sbin 
    $ ./start-all.sh

  # ê°ê° ë”°ë¡œ ì‹œì‘ 
  ## HDFS 
    $ cd $HADOOP_HOME/sbin 
    $ ./start-dfs.sh

  ## YARN   
    $ cd $HADOOP_HOME/sbin 
    $ ./start-yarn.sh 

ã… HDFS Web UI : http://<Master IP>:50070
ã… YARN Web UI : http://<Master IP>:8088


6. HDFS , YARN ì¢…ë£Œ  
  # í•œêº¼ë²ˆì— ì¢…ë£Œ
    $ cd $HADOOP_HOME/sbin 
    $ ./stop-all.sh

  # ê°ê° ë”°ë¡œ ì¢…ë£Œ ( yarn ë¨¼ì € ì¢…ë£Œ ?ğŸ¤”)
  ## YARN 
    $ cd $HADOOP_HOME/sbin 
    $ ./stop-yarn.sh

  ## HDFS 
    $ cd $HADOOP_HOME/sbin 
    $ ./stop-dfs.sh


# ì´ìŠˆ 
  > namenode ê°€ ì¼œì§€ì§€ ì•ŠëŠ” ì´ìŠˆ ì¡´ì¬ (50070í¬íŠ¸ ì›¹í˜ì´ì§€ ì ‘ì† ì•ˆë¨)
  // sbin í´ë”ì—ì„œ 
  ./hadoop-daemon.sh start datanode 
```

## hadoop ëª…ë ¹ì–´ ì…ë ¥ì‹œ WARING ë¡œê·¸ ê´€ë ¨ 

```bash
root@master:/# hadoop fs -ls / 
WARNING: An illegal reflective access operation has occurred 
WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/home/hadoop_home/hado
op-2.10.2/share/hadoop/common/lib/hadoop-auth-2.10.2.jar) to method sun.security.krb5.Config.getInstance()
WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations 
WARNING: All illegal access operations will be denied in a future release 
```

#### ê´€ë ¨ stackoverflow
[https://stackoverflow.com/questions/52155078/how-to-fix-hadoop-warning-an-illegal-reflective-access-operation-has-occurred-e](https://stackoverflow.com/questions/52155078/how-to-fix-hadoop-warning-an-illegal-reflective-access-operation-has-occurred-e)

> ìš”ì•½í•˜ìë©´, java versionê³¼ ë¼ì´ë¸ŒëŸ¬ë¦¬(hadoop-auth-2.10.2.jar) ê´€ë ¨ë˜ì–´ ì¶œë ¥í•˜ëŠ” ë¡œê·¸ë¡œ ì§ì ‘ ì„¤ì • ê³ ì¹  ìˆ˜ ìˆëŠ”ê²Œ ì—†ìŒ. ë‹¨, hadoop 3.x ë²„ì „ë¶€í„° fixed ë˜ì—ˆë‹¤í•¨.

#### hadoop log level ë³€ê²½ì— ëŒ€í•´ ì°¾ì•„ë³´ì. -> ê·¼ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë°œìƒí•˜ëŠ” ë¡œê·¸ì¸ë° ì œì–´ê°€ ê°€ëŠ¥í•œê°€ ì‹¶ìŒ.


#### í•˜ë‘¡ ì—ëŸ¬ 
hadoop fs -mkdir -p /hadoop-dir/mydir01
hadoop fs -mkdir -p /hadoop-dir/mydir02

echo "this is test file 01" > testfile01.txt
echo "this is test file 02" > testfile02.txt

hadoop fs -copyFromLocal testfile01.txt /hadoop-dir/mydir01          // ì•„ë˜ ì—ëŸ¬ ì¶œë ¥ 
hadoop fs -copyFromLocal testfile01.txt /hadoop-dir/mydir02
```
- ë°ì´í„° ë…¸ë“œê°€ ì—†êµ¬ë‚˜ .. jps í™•ì¸í•´ë³´ë‹ˆ í™•ì¸ 
22/07/06 05:09:52 WARN hdfs.DataStreamer: DataStreamer Exception
eplicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation

# ì°¸ê³ 
https://stackoverflow.com/questions/26545524/there-are-0-datanodes-running-and-no-nodes-are-excluded-in-this-operation?answertab=trending#tab-top
https://stackoverflow.com/questions/11889261/datanode-process-not-running-in-hadoop
```

hadoop fs -ls -R /                           // ë‚˜ë‹ˆëª¨ ë‚˜ìº‡ë‹¤ 

```
ì¼ë‹¨ hadoop namenode -format ì„ ì¬ì‹¤í–‰í•˜ë©´ 
/tmp ì— ìƒì„±ë˜ëŠ” h**_ìœ ì €ì•„ì´ë”” í´ë”ì‚­ì œ í•´ì£¼ê³ .. (ì¤‘ìš”í•œë“¯..)
ë‹¤ ì‚­ì œ í›„ ì¬ ìƒì„± 
/home/hadoop_home/datanode
/home/hadoop_home/namenode 
/home/hadoop_home/temp 

chmod -R 755 datanode 
chmod -R 755 namenode 
chmod -R 755 temp 

hadoop namenode -format        // tmp í´ë” ì•ˆì— ë˜ ìƒì„±í•´ì¤Œ

> ./hadoop-daemon.sh start datanode        // ***************** datanode ì˜¬ë¼ì˜´

í•˜ë‘¡ ë¡œê·¸ ê´€ë ¨ ì°¸ê³  >> https://myeonguni.tistory.com/1472
https://www.edureka.co/community/28196/datanode-process-not-running-in-hadoop


// ë‹¹ì—°íˆ í•˜ë‘¡ì— ì˜¬ë¼ì™€ ìˆì–´ì•¼ í•˜ëŠ”ë°.. 
hadoop jar /home/hadoop_home/hadoop-2.10.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.2.jar wordcount /hadoop-dir/mydir01/testfile01.txt wordcount_output

ì—ëŸ¬..) ëŠë‚Œì´ slave1ì—ì„œëŠ” masterì˜ ssh í‚¤ê°€ ì—†ì–´ì„œ.. slaveë¼ë¦¬ êµí™˜í–ˆëŠ”ë° masterëŠ” ì•ˆë˜ë…¸ ;; ssh-copy-id root@master
java.net.ConnectException: Call From slave1/172.17.0.4 to localhost:9000 failed on connection exception: java.net.ConnectExce
ption: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused

> vim /etc/ssh/sshd_config  âœ¨âœ¨ğŸ¤”
  // ìˆ˜ì •í–ˆëŠ”ë° ë°˜ì˜ì´ .. 
  PermitRootLogin yes  
  PasswordAuthentication yes 

> /etc/init.d/ssh restart 
> netstat -tnlp | grep sshd

// slave1, slave2ì—ì„œ ê°ê° ì‹¤í–‰í•˜ë©´ ë™ì‘í•¨ 
> ssh-copy-id -f root@master

// ìš°ìš± .. wordcount ì‹¤í–‰í•´ë„ ì—ëŸ¬ 
  > vim /home/hadoop_home/hadoop-2.10.2/etc/hadoop/core-site.xml   // localhost:9000 ì„ master:9000ìœ¼ë¡œ ìˆ˜ì •

  > ./stop-all.sh
  > ./start-all.sh 

ì—ëŸ¬)org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /tmp/hadoop-yarn/staging/root/.staging/job_1657090927  Name node is in safe mode.
  // safe mode ì¢…ë£Œ 
  > hdfs dfsadmin -safemode leave 
  ë˜ëŠ” 
  > hdfs dfsadmin -safemode forceExit
  // ìƒíƒœ í™•ì¸
  > hdfs dfsadmin -safemode get


ì—ëŸ¬)  DataStreamer Exception
eplicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation
// ë°ì´í„° ë…¸ë“œ ì¬ì‹œì‘ ($HADOOP_HOME/sbin) , start-all.sh || stop-all.sh ì™€ ë³„ê°œë¡œ ë™ì‘í•˜ëŠ” ë“¯í•¨
  > ./hadoop-daemon.sh stop datanode 
  > ./hadoop-daemon.sh start datanode 

// ê²°ê³¼ ì¶”ì¶œ ë¨ .. âœ¨ğŸ¤”ğŸ¤”ğŸ˜

  # echo "this is test file 01" > testfile01.txt

  # hadoop fs -mkdir -p /hadoop-dir/mydir01
  # hadoop fs -copyFromLocal testfile01.txt /hadoop-dir/mydir01

  # hadoop fs -ls -R / 

  # hadoop jar /home/hadoop_home/hadoop-2.10.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.2.jar wordcount /hadoop-dir/mydir01/testfile01.txt wordcount_output
  # hadoop fs -text /wordcount/part-r-00000
  01      1
  file    1
  is      1
  test    1
  this    1


ì—ëŸ¬..) íŒŒì¼ ìˆì–´ì„œ ì´ëŸ¼.. 
org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://localhost:9000/user/root/wordcount_output already exists
> hadoop fs -ls /     
> hadoop fs -rmr /wordcount*

ì°¸ê³  ) How to track which data block is in which data node in hadoop?
# hadoop fsck /hadoop-dir/mydir01/testfile01.txt -files -blocks -locations 

```
