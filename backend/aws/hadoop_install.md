## ì°¸ê³  ê¸°ìˆ  ë¸”ë¡œê·¸ 

**wikidoc**
[https://wikidocs.net/37809](https://wikidocs.net/37809)

[https://1mini2.tistory.com/83](https://1mini2.tistory.com/83)

> centos ê¸°ë°˜ì¸ë° .. ë‚˜ëŠ” ec2 ì„œë²„ì— ubuntu ë¡œ í•´ì„œ ëª…ë ¹ì–´ê°€ ë‹¤ë¦„

**í•˜ë‘¡2.10.1 ì„¤ì¹˜**
[https://1mini2.tistory.com/84](https://1mini2.tistory.com/84)
[https://hoing.io/archives/22174](https://hoing.io/archives/22174)

**centos7 ê¸°ë°˜ ì„¤ì¹˜**
[https://www.tecmint.com/install-hadoop-single-node-on-centos-7/](https://www.tecmint.com/install-hadoop-single-node-on-centos-7/)

## DATA ON-AIR í•˜ë‘¡ ì„¤ì¹˜ 1.x, 2.x **
[https://dataonair.or.kr/db-tech-reference/d-guide/data-practical/?pageid=2&mod=document&keyword=hadoop&uid=385](https://dataonair.or.kr/db-tech-reference/d-guide/data-practical/?pageid=2&mod=document&keyword=hadoop&uid=385)


**í•˜ë‘¡NOdeì„¤ëª…**
[https://blog.geunho.dev/posts/hadoop-namenode/#journalnode](https://blog.geunho.dev/posts/hadoop-namenode/#journalnode)

ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ( ubuntuì— ë¬´ë£Œ ì‚¬ì–‘, ìƒëµ )

pem í‚¤ ì‹ ê·œ ìƒì„±í•´ì„œ ì ‘ì†
[https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html 'ec2 ì ‘ì† ê°€ì´ë“œ')

```bash
> cd ~/.ssh 
> cp /mnt/c/Users/Administrator/Desktop/íŒŒì¼ì •ë¦¬/ìŠ¤í„°ë””/sampleKeypair.pem ./ 
> ssh -i sampleKeypair.pem ubuntu@13.125.251.186

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@         WARNING: UNPROTECTED PRIVATE KEY FILE!          @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
Permissions 0755 for 'sampleKeypair.pem' are too open.
It is required that your private key files are NOT accessible by others.       >> 755 ê¶Œí•œì€ ì•ˆì „í•˜ì§€ ëª»í•˜ë‹ˆ ë³€ê²½ í•´ì¤˜ë¼ëŠ” ëœ»ì¸ë“¯
This private key will be ignored.
Load key "sampleKeypair.pem": bad permissions
ubuntu@13.125.251.186: Permission denied (publickey).

> chmod -R 600 ~/.ssh/sampleKeypair.pem
> ssh -i sampleKeypair.pem ubuntu@13.125.251.186      // ì ‘ì† ì„±ê³µâœ¨

> cat /etc/*release                                  // OS í™•ì¸ 
> whoami                                             // ubuntu (ë‹¹ì—°ğŸ˜…)

// ê¸°ë³¸ ì„¤ì •
> sudo apt-get update               
> sudo apt-get install net-tools -y        // ì´ë¯¸ ê¹”ë ¤ìˆëŠ”ê²ƒë„ ìˆìŒ 

> sudo apt-get install openjdk-11-jdk -y 
> java -version

// JAVA_HOME í™˜ê²½ë³€ìˆ˜ ì„¤ì • 
> vi ~/.bashrc                  // centosëŠ” /etc/profile ì— ì„¤ì •í•˜ëŠ”ë“¯ğŸ‘¨â€ğŸ’»

    ----------------------------
    export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))
    export PATH=$PATH:$JAVA_HOME/bin
    ----------------------------

> source ~/.bashrc 
> echo $JAVA_HOME      // í™˜ê²½ë³€ìˆ˜ ì„¤ì • í™•ì¸
```

## í•˜ë‘¡ ì„¤ì¹˜ 
```bash
> sudo su          // rootë¡œ ì „í™˜
> wget https://dlcdn.apache.org/hadoop/common/hadoop-2.10.2/hadoop-2.10.2.tar.gz
> cd /usr/local                //ì—¬ê¸°ì— ì„¤ì¹˜í•˜ë ¤ëŠ”ë“¯

> tar -zxvf ./hadoop-2.10.2.tar.gz -C /usr/local/

> chown -R root:root /usr/local/hadoop-2.10.2/
> ls -al /usr/local/

> vim ~/.bashrc             // ubuntuë¡œ export ì„¤ì •í•œ ë‚´ìš©ì´ ì—†ìŒ.. rootì™€ ubuntu ê°ê° ì„¤ì •íŒŒì¼ì„ êµ¬ë¶„ ì§€ì–´ ì£¼ëŠ” ê²ƒìœ¼ë¡œ íŒë‹¨ 
> exit                      // ubuntu ì „í™˜ 
> vim ~/.bashrc             // realë¡œ ê³„ì • ë³„ë¡œ ì‚¬ìš©ì ì„¤ì •ì´ ë‹¤ëë‹¤ ! 

  ----------------------------
    # hadoop setup
    export HADOOP_HOME=/usr/local/hadoop-2.10.2
    export HADOOP_CONFIG=$HADOOP_HOME/etc/hadoop
    export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))
    export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin/:$HADOOP_HOME/sbin
  ----------------------------

> source ~/.bashrc 
> echo $HADOOP_HOME
    /usr/local/hadoop-2.10.2
> echo $HADOOP_CONFIG
    /usr/local/hadoop-2.10.2/etc/hadoop

// hadoop-env.sh ì„¤ì • ìˆ˜ì • 
$ echo $JAVA_HOME
    /usr/lib/jvm/java-11-openjdk-amd64
$ sudo vim $HADOOP_CONFIG/hadoop-env.sh    
    ----------------------------
    export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
    ----------------------------
```

## í•˜ë‘¡ ê³µì‹ ì‚¬ì´íŠ¸ (Deprecated Properties)
[https://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-common/DeprecatedProperties.html](https://hadoop.apache.org/docs/r2.10.1/hadoop-project-dist/hadoop-common/DeprecatedProperties.html)
[https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/DeprecatedProperties.html](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/DeprecatedProperties.html)

## í•˜ë‘¡ ì„¤ì • íŒŒì¼ ì„¤ì •í•˜ê¸°
1. core-site.xml

  > ë¡œê·¸íŒŒì¼, ë„¤íŠ¸ì›Œí¬ íŠœë‹, I/O íŠœë‹, íŒŒì¼ì‹œìŠ¤í…œ íŠœë‹, ì••ì¶• ë“± ì‹œìŠ¤í…œ ì„¤ì • íŒŒì¼
  > HDFSì™€ MapReduceì—ì„œ ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©í•  í™˜ê²½ ì •ë³´ë¥¼ ì…ë ¥í•˜ê²Œ ë˜ë©°, core-default.xmlì´ ê¸°ë³¸ê°’ì´ê³ , core-site.xmlí†µí•´ ì„¤ì • override í•˜ì—¬ ì‚¬ìš©í•¨
  > ë§¤ê°œë³€ìˆ˜ ì •ë³´ : https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/core-default.xml

  $ sudo vim $HADOOP_CONFIG/core-site.xml     
  
  ```xml
    // NameNode ë°ëª¬ì´ ì‹¤í–‰ì¤‘ì¸ ìœ„ì¹˜ ë‚˜íƒ€ëƒ„. master:9000 ì„œë²„ì—ì„œ NameNode ì‹¤í–‰ì¤‘
    <configuration>
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://ec2-54-180-80-163.ap-northeast-2.compute.amazonaws.com:9000</value>       <!--aws ec2ì—ì„œëŠ” public dbs ìœ¼ë¡œ í•´ì•¼í•¨ -->
        </property>
    </configuration>
  ```



2. hdfs-site.xml
  
  > HDFSì—ì„œ ì‚¬ìš©í•  í™˜ê²½ì •ë³´ ì„¤ì •. hdfs-default.xmlì´ ê¸°ë³¸ê°’ì´ë©°, hdfs-site.xml í†µí•´ ì„¤ì • override í•˜ì—¬ ì‚¬ìš©í•¨ 
  > ë§¤ê°œë³€ìˆ˜ ì •ë³´ : https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml

  $ sudo vim $HADOOP_CONFIG/hdfs-site.xml

  ```xml 
    <configuration>
        <property>
            <name>dfs.namenode.name.dir</name>
            <value>file:///data/namenode</value> 
            <!-- NameNodeì—ì„œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì™€ íŠ¸ëœì­ì…˜ ë¡œê·¸ë¥¼ ì§€ì†ì ìœ¼ë¡œ ì €ì¥í•˜ëŠ” ë¡œì»¬íŒŒì¼ ì‹œìŠ¤í…œ ê²½ë¡œ (fsImage, editlog)-->
        </property>
        <property>
            <name>dfs.datanode.data.dir</name>
            <value>file:///data/datanode</value> 
            <!-- ì‹¤ì œ ë°ì´í„°ë¥¼ ì €ì¥í•  ë°ì´í„° ë…¸ë“œì˜ ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ ê²½ë¡œ -->
        </property>
        <property>
            <name>dfs.namenode.checkpoint.dir</name>
            <value>file:///data/namesecondary</value> 
            <!-- DFS ë³´ì¡° ë„¤ì„ë…¸ë“œê°€ ë³‘í•©ë˜ì–´ì•¼í•  ì„ì‹œ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•˜ëŠ” ë¡œì»¬íŒŒì¼ ì‹œìŠ¤í…œì˜ ê²½ë¡œ-->
        </property>
        <property>
            <name>dfs.replication</name>
            <value>3</value> <!-- ë¸”ë¡ ë³µì œ ìˆ˜ë¥¼ ì§€ì •-->
        </property>
    </configuration>
  ``` 

3. mapred-site.xml

   > MapReduce ì–´í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •íŒŒì¼ 
   > ë§¤ê°œë³€ìˆ˜ ì •ë³´ : https://hadoop.apache.org/docs/r2.7.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml


   $ sudo vim $HADOOP_CONFIG/mapred-site.xml

   ```xml
        <configuration>
            <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
                <!-- MapReduce ì‹¤í–‰ í”„ë ˆì„ì›Œí¬ë¥¼ Hadoop YARNìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.-->
            </property>
        </configuration>
   ```

4. yarn-site.xml 

  > Resource Manager ë° Node Manager ì— ëŒ€í•œ êµ¬ì„±ì„ ì •ì˜í•¨ 
  > ë§¤ê°œë³€ìˆ˜ ì •ë³´ : https://hadoop.apache.org/docs/r2.7.6/hadoop-yarn/hadoop-yarn-common/yarn-default.xml

  $ sudo vim $HADOOP_CONFIG/yarn-site.xml

  ```xml
    <configuration>
        <property>
            <name>yarn.nodemanager.local-dirs</name>
            <value>file:///data/yarn/local</value>
            <!-- ì¤‘ê°„ ë°ì´í„°ê°€ ê¸°ë¡ë  ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì˜ ëª©ë¡ì…ë‹ˆë‹¤. ë‹¤ì¤‘ ê²½ë¡œëŠ” disk I/Oë¥¼ ë¶„ì‚°í•˜ëŠ”ë° ë„ì›€ë¨-->
        </property>
        <property>
            <name>yarn.nodemanager.log-dirs</name>
            <value>file:///data/yarn/logs</value>
            <!-- ë¡œê·¸ê°€ ê¸°ë¡ë  ë¡œì»¬íŒŒì¼ ì‹œìŠ¤í…œì˜ ëª©ë¡. ë‹¤ì¤‘ ê²½ë¡œëŠ” disk I/O ë¶„ì‚°í•˜ëŠ”ë° ë„ì›€ë¨(?)-->
        </property>
        <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>ip-172-31-41-173.ap-northeast-2.compute.internal</value>       <!--aws ì—ì„œëŠ” private dns ë¥¼ ì ì–´ì¤˜ì•¼í•¨ , ìš”ê±°ë§Œ ì ì–´ì£¼ë©´ ë‚˜ë¨¸ì§€ íŒŒëŒì€ í¬íŠ¸ë§Œ ë°”ë€ŒëŠ”ê±¸ë¡œ ë³€ê²½ë¨ 2.10.2-->
            <!-- ë¦¬ì†ŒìŠ¤ ë§¤ë‹ˆì € ë°ëª¬ì´ ë™ì‘í•˜ê³  ì‡ëŠ” í˜¸ìŠ¤íŠ¸ ìœ„ì¹˜-->
        </property>

        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>
        <property>
            <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
            <value>org.apache.hadoop.mapred.ShuffleHandler</value>
        </property>

    </configuration>
  ```

## EC2 ë³µì œí•˜ê¸° 
- í˜„ì¬ ì„¤ì •í•´ë‘” Master.hadoop ê¸°ì¤€ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„±
- ìƒì„±í•œ ì´ë¯¸ì§€ë¡œ Slave.hadoop ì¸ìŠ¤í„´ìŠ¤ 3ê°œ ìƒì„± 
  - AMI ì´ë¯¸ì§€ ìƒì„±ì‹œ "AMI is xxxx pending, and cannot be run" ì—ëŸ¬ ì¶œë ¥ 
  - AMI ì´ë¯¸ì§€ íƒ¬í”Œë¦¿ ìƒì„± í›„ ë‹¤ì‹œ ë§Œë“¤ë©´ ë˜ê¸°ëŠ” í•¨.


## Master ì„œë²„ ì„¤ì • 
1. Hostname ë³€ê²½ 
   $ sudo hostnamectl set-hostname master.hadoop 
   $ hostnamectl

     Static hostname: master.hadoop
       Icon name: computer-vm
         Chassis: vm
      Machine ID: a51cc3f7851142cc859828bfae9edd56
         Boot ID: f10ee558768d47ed9b1ea49ba6bd7261
  Virtualization: xen
Operating System: Ubuntu 22.04 LTS
          Kernel: Linux 5.15.0-1011-aws
    Architecture: x86-64
 Hardware Vendor: Xen
  Hardware Model: HVM domU

2. /etc/hosts íŒŒì¼ ìˆ˜ì • 
    
   $ sudo vim /etc/hosts 
    // public dns (ì´ê±¸ë¡œ ì„¤ì •í•´ì•¼ í•¨ !! master / slave )
    172.31.41.173 ec2-54-180-80-163.ap-northeast-2.compute.amazonaws.com master.hadoop master
    172.31.34.225 ec2-15-164-163-43.ap-northeast-2.compute.amazonaws.com slave1.hadoop slave1
    172.31.45.115 ec2-54-180-109-71.ap-northeast-2.compute.amazonaws.com slave2.hadoop slave2
    172.31.42.121 ec2-15-164-212-171.ap-northeast-2.compute.amazonaws.com slave3.hadoop slave3

    // private dns 
    ping ip-172-31-41-173.ap-northeast-2.compute.internal  
    ping ip-172-31-34-225.ap-northeast-2.compute.internal  
    ping ip-172-31-45-115.ap-northeast-2.compute.internal  
    ping ip-172-31-42-121.ap-northeast-2.compute.internal  

3. ssh í‚¤ ìƒì„± ë° ssh ì„¤ì • ë³€ê²½ 
    
    $ sudo ssh-keygen            // í…ŒìŠ¤íŠ¸ ìš©ë„ë¼ ê°„ë‹¨í•˜ê²Œ ìƒì„± 
    $ sudo vi /etc/ssh/sshd_config 
    
    ```bash
        PermitRootLogin yes  
        PasswordAuthentication yes 
    ```
    $ sudo /etc/init.d/ssh restart      // ubuntuì¸ ê²½ìš° , linuxì¸ ê²½ìš° sudo systemctl restart sshd
    $ sudo netstat -tnlp | grep sshd

4. root ë¹„ë°€ë²ˆí˜¸ ë³€ê²½
   $ sudo passwd root          // í†µê³„ëŠ”êµ¬ë¼ë‹¤!! 

5. Slaves íŒŒì¼ ì„¤ì •
   
   > Master ì„œë²„ì—ì„œë§Œ ì‹¤í–‰ 

   $ sudo vim $HADDOP_CONFIG/slaves 

   // ì¶”ê°€ í›„ ì €ì¥(public aws dns ì¶”ê°€í•´ì¤˜ì•¼ í•¨)
   ec2-15-164-163-43.ap-northeast-2.compute.amazonaws.com 
   ec2-54-180-109-71.ap-northeast-2.compute.amazonaws.com
   ec2-15-164-212-171.ap-northeast-2.compute.amazonaws.com

--- 

## Slave 1~3 ì„œë²„ ì„¤ì • 
   $ cd ~/.ssh 
   slave1 >> ssh -i sampleKeypair.pem ubuntu@15.164.170.18
   slave2 >> ssh -i sampleKeypair.pem ubuntu@13.124.203.216
   slave3 >> ssh -i sampleKeypair.pem ubuntu@3.34.130.163

1. Hostname ë³€ê²½ 

   $ sudo hostnamectl set-hostname slave1.hadoop 
   $ hostnamectl

2. /etc/hosts ìˆ˜ì • (ë™ì¼)
3. ssh í‚¤ ìƒì„± ë° ssh ì„¤ì • ë³€ê²½ (ë™ì¼)
4. root ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ (ë™ì¼)

5. (ê³µí†µ) master ì„œë²„ì™€ key êµí™˜ 
    $ sudo su 
    $ ssh-copy-id root@master 

6. masterì—ì„œëŠ” slave1,2,3 ssh key ê°€ì ¸ì˜¤ê¸° 
    $ sudo su
    $ ssh-copy-id root@slave1 
    $ ssh-copy-id root@slave2 
    $ ssh-copy-id root@slave3 



## Master ì„œë²„ì—ì„œ NameNode í¬ë§· 

    $ /usr/local/hadoop-2.10.2/bin/hdfs namenode -format

    > hdfs-site.xml ì—ì„œ ì„¤ì •í•œ ëŒ€ë¡œ /data/namenode í´ë” ìƒì„±ë˜ê³  ì´ˆê¸°í™”ë¨

 ## ê°ê° slave 1,2,3 ì„œë²„ì—ì„œ dataNodeí¬ë§· 

     $ /usr/local/hadoop-2.10.2/bin/hdfs datanode -format  

 ## Master ì„œë²„ì—ì„œ HDFSì‹œì‘
    $ /usr/local/hadoop-2.10.2/sbin/start-dfs.sh         // namenodeì™€ scenodaryNameNode ë…¸ë“œ ì˜¬ë¼ì˜¤ 

    ì—ëŸ¬)java.net.BindException: Problem binding to [master:9000] java.net.BindException: Cannot assign requested address; For more details see:  http://wiki.apache.org/hadoop/BindException      
    > If you are running on EC2, your service is trying to explicitly bind a public Elastic IP address using the public hostname or IP, or implicitly using "0.0.0.0" as the address.

    > core-site.xml ì—ì„œ master:9000 ëŒ€ì‹  0.0.0.0:9000 í•˜ê³  ì¬ì‹œì‘í•˜ë‹ˆ ë¨ 

    ì—ëŸ¬) ë§ˆìŠ¤í„°IP:50070 ì ‘ì† ì•ˆë¨ 

    > AWS ec2 ì˜ ì¸ë°”ìš´ë“œ ê·œì¹™ ìˆ˜ì • í•„ìš” 
    > https://inkkim.github.io/hadoop/AWS-EC2%EC%97%90%EC%84%9C-Hadoop-Cluster-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0/

## yarn ì‹œì‘ 
    $ /usr/local/hadoop-2.10.2/sbin/start-yarn.sh       // resourceManage(MASTER)/nodeManager(slave*) ê°ê° ì˜¬ë¼ì™€ì•¼í•¨

    ì—ëŸ¬) resourceManagerê°€ ì•ˆì˜¬ë¼ì˜´ -> slog ì— resource ë§¤ë‹ˆì € ê´€ë ¨ íŒŒì¼ì´ ìˆìŒ .. 

    2022-07-07 04:59:34,893 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error starting ResourceManager org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server
    Caused by: java.net.BindException: Port in use: master:8088

    > yarn-site.xmlì— ë³µë¶™í• ë•Œ configuration íƒœê·¸ê°€ í•œê²¹ ë” ë“¤ì–´ê°”ì—ˆìŒ..
    > https://hadoop.apache.org/docs/r2.10.2/hadoop-yarn/hadoop-yarn-common/yarn-default.xml
    > ë³´ë‹ˆê¹ hostname ì´ master.hadoopì´ë“  masterë“  ëª»ì°¾ë„¤..  0.0.0.0 ìœ¼ë¡œ ìˆ˜ì •í•˜ë‹ˆ ì •ìƒë™ì‘í•¨
    > ì°¸ê³  https://stackoverflow.com/questions/22846028/hadoop-2-2-0-resourcemanager-fails-to-bind-to-port-8088
        >> ì„¤ì •ì—ì„œ í¬íŠ¸ë¥¼ ë°•ì•„ë²„ë¦¬ë‹ˆ.. 

./start-all.sh ì‹œì‘í•˜ë ¤ë‹ˆ 
deprecated ë˜ê³  start-dfs.sh ì™€ start-yarn.sh ê°€ ëŒ€ì‹  ì‹¤í–‰ëœë‹¤ê³  ë¡œê·¸ ëœ¸     


http://13.125.251.186:8088
http://13.125.251.186:50070

## íƒœìŠ¤íŠ¸ í•˜ëŠ”ë° ì•ˆëœë‹¤.. 
- core-site.xmlì— ê¸°ë³¸ í¬íŠ¸ê°€ 8020 ì´ì—ˆìŒ 
    INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8020. Already tried
    > core-site.xml ì— í¬íŠ¸ë¥¼ 9000ìœ¼ë¡œ ìˆ˜ì •í•¨

    - ê° ì„œë²„ì— core-site.xml , yarn-site.xml ìˆ˜ì •í•¨.. 
        - yarn-siteì— íƒœê·¸ ì˜ëª» ë“¤ì–´ê°„ê±°.. 
        - core-site.xmlì— master ì¸ì‹ ëª»í•˜ëŠ”ê±° ìˆ˜ì • -> 13.125.251.186

        org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/13.125.251.186:8031.

- í¬ë§· ë¶€í„° ê¼¬ì¸ê±° ê°™ìŒ.. ì„¤ì •ì´ ì˜ëª»ë˜ì—‡ëŠ”ë° .. 
  /data í´ë” ê° ì„œë²„ ë³„ë¡œ ë‹¤ ì§€ìš°ê³  , format ë‹¤ì‹œí•˜ì          

   $ /usr/local/hadoop-2.10.2/bin/hdfs namenode -format
    $ /usr/local/hadoop-2.10.2/bin/hdfs datanode -format  

    ì—ëŸ¬) 
    ssh: Could not resolve hostname slave3: Temporary failure in name resolution
    ssh: Could not resolve hostname slave2: Temporary failure in name resolution
    ssh: Could not resolve hostname slave1: Temporary failure in name resolution


    wordcount ì‹¤í–‰í•˜ë‹ˆ.. resourcemanager ê¼¬ë¼ì§€ê°€ ì´ëŸ¼ ..
    ì—ëŸ¬) 22/07/07 09:17:40 INFO client.RMProxy: Connecting to ResourceManager at ip-172-31-41-173.ap-northeast-2.compute.internal/172.3
    > yarn-site.xml
    > https://hadoop.apache.org/docs/r2.10.2/hadoop-yarn/hadoop-yarn-common/yarn-default.xml 
    > ì„¤ëª… ë³´ë©´ í¬íŠ¸ëŠ” ê³ ì •ì´ë¼ì„œ hostnameë§Œ ë„£ì–´ì£¼ë©´ ë¨
    > 8088 í¬íŠ¸ ì•ˆì˜¬ë¼ì˜´ 

    org.apache.hadoop.io.retry.RetryInvocationHandler: org.apache.hadoop.net.ConnectTimeoutException:
    Call From master.hadoop/13.125.251.186 to slave2.hadoop:38147 failed on socket timeout exception: org.apache.hadoop.net.Conne ctTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChann 
    el[connection-pending remote=slave2.hadoop/13.124.203.216:38147]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout, while invoking ContainerManagementProtocolPBClientImpl.startContainers over null. Retrying after sleeping for 10000ms.


    ì—ëŸ¬) 
    hdfs.DataStreamer: Caught exception java.lang.InterruptedException

    ì—ëŸ¬) 
    22/07/07 10:04:12 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:10020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)


    java.io.IOException: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort

    >> https://stackoverflow.com/questions/17930644/connection-error-in-apache-pig
    >> mapreduce.jobhistory.address	|| 0.0.0.0:10020    ê¸°ë³¸ ì„¤ì •ì´ ë˜ì–´ìˆë„¤ 2.10.2 ë¶€í„° 
    
    $ mr-jobhistory-daemon.sh start historyserver        // í•˜ë©´ ì˜¬ë¼ì˜¤ëŠ”ë°..


    >> /etc/hosts íŒŒì¼ê³¼ hadoop ì„¤ì •íŒŒì¼ì—ì„œ namenode ì£¼ì†ŒëŠ” aws public dns ë¥¼ ì‚¬ìš©í•´ì•¼í–ˆìŒ..(ì •ë‹µ)

    ì—ëŸ¬) Container launch failed for container_1657242159385_0001_01_000004 : org.apache.hadoop.yarn.exceptions.InvalidAuxServiceException: The auxService:mapreduce_shuffle does not exist

    > yarn-site.xml ì— ì¶”ê°€ (https://stackoverflow.com/questions/30921838/auxservicemapreduce-shuffle-does-not-exist-on-hive)

     <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>

    yarn.nodemanager.aux-services.mapreduce_shuffle.class ëŠ” default ê°™ì€ë°..

# ìµœì¢… í…ŒìŠ¤íŠ¸ ì»¤ë§¨ë“œ 
 ```

$ cd /usr/local/hadoop-2.10.2/bin

$ ./hadoop fs -ls -R /
$ ./hadoop fs -mkdir /anderon 

$ echo "this is test file 01" > testfile01.txt
$ ./haddop fs -copyFromLocal testfile01.txt /anderson

$ ./hadoop fs -ls -R /anderson

$ ./hadoop jar /usr/local/hadoop-2.10.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.2.jar wordcount /anderson/testfile01.txt /result

$ ./hadoop fs -text /result/íŒŒì¼ëª…          // ê²°ê³¼ í™•ì¸

 ```   



 /usr/local/hadoop-2.10.2/bin/hadoop namenode -format
/usr/local/hadoop-2.10.2/bin/hadoop datanode -format
